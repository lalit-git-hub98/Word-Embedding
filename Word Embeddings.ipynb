{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1fB4xy370bo7GyH2CCuAjaGrcT_35Xqy2","authorship_tag":"ABX9TyMgzVymF4CD7+Dir0IdsspZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# One Hot Encoding"],"metadata":{"id":"SBQBxZJ6pxo9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8b_DKNvkH70"},"outputs":[],"source":["import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder"]},{"cell_type":"markdown","source":["### One Hot Encoding --> using sklearn"],"metadata":{"id":"2fW2dQevq0dZ"}},{"cell_type":"code","source":["# Read the data\n","df = pd.read_csv(\"/content/drive/MyDrive/NLP Techniques/Car_Details.csv\")\n","\n","# One-Hot Encoding for 'fuel', 'seller_type', and 'transmission'\n","one_hot_encoder = OneHotEncoder(sparse_output=False)\n","encoded_features = one_hot_encoder.fit_transform(df[[\"fuel\", \"seller_type\", \"transmission\"]])\n","one_hot_df = pd.DataFrame(encoded_features, columns=one_hot_encoder.get_feature_names_out([\"fuel\", \"seller_type\", \"transmission\"]))\n","\n","# Combine the encoded features with the original DataFrame\n","df = df.drop(columns=[\"fuel\", \"seller_type\", \"transmission\"])\n","df = pd.concat([df, one_hot_df], axis=1)\n","\n","# Display the resulting DataFrame\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVBYCzWOp7xL","executionInfo":{"status":"ok","timestamp":1744036363411,"user_tz":240,"elapsed":112,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"28e46e0f-892b-4417-db55-91d554b5fdac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                       name  year  selling_price  km_driven         owner  \\\n","0             Maruti 800 AC  2007          60000      70000   First Owner   \n","1  Maruti Wagon R LXI Minor  2007         135000      50000   First Owner   \n","2      Hyundai Verna 1.6 SX  2012         600000     100000   First Owner   \n","3    Datsun RediGO T Option  2017         250000      46000   First Owner   \n","4     Honda Amaze VX i-DTEC  2014         450000     141000  Second Owner   \n","\n","   fuel_CNG  fuel_Diesel  fuel_Electric  fuel_LPG  fuel_Petrol  \\\n","0       0.0          0.0            0.0       0.0          1.0   \n","1       0.0          0.0            0.0       0.0          1.0   \n","2       0.0          1.0            0.0       0.0          0.0   \n","3       0.0          0.0            0.0       0.0          1.0   \n","4       0.0          1.0            0.0       0.0          0.0   \n","\n","   seller_type_Dealer  seller_type_Individual  seller_type_Trustmark Dealer  \\\n","0                 0.0                     1.0                           0.0   \n","1                 0.0                     1.0                           0.0   \n","2                 0.0                     1.0                           0.0   \n","3                 0.0                     1.0                           0.0   \n","4                 0.0                     1.0                           0.0   \n","\n","   transmission_Automatic  transmission_Manual  \n","0                     0.0                  1.0  \n","1                     0.0                  1.0  \n","2                     0.0                  1.0  \n","3                     0.0                  1.0  \n","4                     0.0                  1.0  \n"]}]},{"cell_type":"markdown","source":["### One Hot Encoding --> From Scratch"],"metadata":{"id":"cYDDkkfBq6G8"}},{"cell_type":"code","source":["def one_hot_encode(df, columns):\n","    \"\"\"\n","    Perform one-hot encoding on specified columns of a DataFrame.\n","\n","    Args:\n","    df (pd.DataFrame): The DataFrame to encode.\n","    columns (list): A list of columns to one-hot encode.\n","\n","    Returns:\n","    pd.DataFrame: The DataFrame with one-hot encoded columns.\n","    \"\"\"\n","    for column in columns:\n","        # Get unique values\n","        unique_values = df[column].unique()\n","\n","        # Create a binary column for each unique value\n","        for value in unique_values:\n","            df[f\"{column}_{value}\"] = (df[column] == value).astype(int)\n","\n","        # Drop the original column\n","        df.drop(column, axis=1, inplace=True)\n","\n","    return df\n","\n","# Read the data\n","df = pd.read_csv(\"/content/drive/MyDrive/NLP Techniques/Car_Details.csv\")\n","\n","# Apply one-hot encoding\n","df = one_hot_encode(df, [\"fuel\", \"seller_type\", \"transmission\"])\n","\n","# Display the resulting DataFrame\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXhATpMuqqAU","executionInfo":{"status":"ok","timestamp":1744036516039,"user_tz":240,"elapsed":89,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"25a02ab4-1d59-4cbe-a4d3-23bd2d337ed3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                       name  year  selling_price  km_driven         owner  \\\n","0             Maruti 800 AC  2007          60000      70000   First Owner   \n","1  Maruti Wagon R LXI Minor  2007         135000      50000   First Owner   \n","2      Hyundai Verna 1.6 SX  2012         600000     100000   First Owner   \n","3    Datsun RediGO T Option  2017         250000      46000   First Owner   \n","4     Honda Amaze VX i-DTEC  2014         450000     141000  Second Owner   \n","\n","   fuel_Petrol  fuel_Diesel  fuel_CNG  fuel_LPG  fuel_Electric  \\\n","0            1            0         0         0              0   \n","1            1            0         0         0              0   \n","2            0            1         0         0              0   \n","3            1            0         0         0              0   \n","4            0            1         0         0              0   \n","\n","   seller_type_Individual  seller_type_Dealer  seller_type_Trustmark Dealer  \\\n","0                       1                   0                             0   \n","1                       1                   0                             0   \n","2                       1                   0                             0   \n","3                       1                   0                             0   \n","4                       1                   0                             0   \n","\n","   transmission_Manual  transmission_Automatic  \n","0                    1                       0  \n","1                    1                       0  \n","2                    1                       0  \n","3                    1                       0  \n","4                    1                       0  \n"]}]},{"cell_type":"markdown","source":["## Pros and Cons of One-Hot Encoding\n","\n","### Pros of One-Hot Encoding\n","\n","- **No Ordinal Relationships**: One-hot encoding does not impose any ordinal relationship between the categories. Each category is treated independently, which is beneficial when there is no inherent order in the categories.\n","\n","- **Simplicity**: One-hot encoding is straightforward to implement and understand. It creates binary columns that are easy to interpret.\n","\n","\n","### Cons of One-Hot Encoding\n","\n","- **Increased Dimensionality**: One-hot encoding can significantly increase the dimensionality of the dataset, especially if there are many unique categories. This can lead to the \"curse of dimensionality\" and make the model more complex and slower to train.\n","\n","- **Sparsity**: The resulting matrix is often sparse, meaning most of the elements are zero. This can be inefficient in terms of memory usage and computation. It can also cause overfitting.\n","\n","- **Multicollinearity**: One-hot encoding can introduce multicollinearity, especially if one of the categories is perfectly predictable from the others. This can affect the performance of some algorithms, like linear regression.\n","\n","- **Not Suitable for High Cardinality**: For categorical variables with high cardinality (many unique values), one-hot encoding can become impractical due to the large number of resulting columns.\n","\n","- **Interpretability**: While one-hot encoding is simple, the interpretation of the resulting model can become complex, especially when dealing with a large number of categories.\n","\n","- **OOV**: It cannot handle out of vocabulary words.\n","\n","- **Semantic Meaning**: The way words are converted to vectors, all the vectors are equidistant from each other. So, semantic meaning cannot be captured from this.\n","\n","### When to Use One-Hot Encoding\n","\n","- Use one-hot encoding when the categorical variable does not have an inherent order and when the number of unique categories is manageable.\n","- It is particularly useful for algorithms that do not handle categorical data natively, such as linear models and neural networks.\n"],"metadata":{"id":"57RSw9PZy-0W"}},{"cell_type":"markdown","source":["# Label Encoding"],"metadata":{"id":"MNPKIuA3rXe_"}},{"cell_type":"markdown","source":["### Label Encoding --> Using Sklearn"],"metadata":{"id":"EBCTzC6yr0TE"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder"],"metadata":{"id":"nZK0GW-mrU-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read the data\n","df = pd.read_csv(\"/content/drive/MyDrive/NLP Techniques/Car_Details.csv\")\n","\n","# Label Encoding for 'owner'\n","label_encoder = LabelEncoder()\n","df[\"owner\"] = label_encoder.fit_transform(df[\"owner\"])\n","\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NaHjH777rfwG","executionInfo":{"status":"ok","timestamp":1744036604127,"user_tz":240,"elapsed":46,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"a54e7954-6389-4b2f-c3d0-5cf4065cf3ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                       name  year  selling_price  km_driven    fuel  \\\n","0             Maruti 800 AC  2007          60000      70000  Petrol   \n","1  Maruti Wagon R LXI Minor  2007         135000      50000  Petrol   \n","2      Hyundai Verna 1.6 SX  2012         600000     100000  Diesel   \n","3    Datsun RediGO T Option  2017         250000      46000  Petrol   \n","4     Honda Amaze VX i-DTEC  2014         450000     141000  Diesel   \n","\n","  seller_type transmission  owner  \n","0  Individual       Manual      0  \n","1  Individual       Manual      0  \n","2  Individual       Manual      0  \n","3  Individual       Manual      0  \n","4  Individual       Manual      2  \n"]}]},{"cell_type":"markdown","source":["### Label Encoding --> From Scratch"],"metadata":{"id":"h1UpalCjruNt"}},{"cell_type":"code","source":["def label_encode(df, column):\n","    \"\"\"\n","    Perform label encoding on a specified column of a DataFrame.\n","\n","    Args:\n","    df (pd.DataFrame): The DataFrame to encode.\n","    column (str): The column to label encode.\n","\n","    Returns:\n","    pd.DataFrame: The DataFrame with the label encoded column.\n","    \"\"\"\n","    # Get unique values and sort them\n","    unique_values = sorted(df[column].unique())\n","\n","    # Create a mapping from value to integer\n","    value_to_int = {value: i for i, value in enumerate(unique_values)}\n","\n","    # Map the values to integers\n","    df[column] = df[column].map(value_to_int)\n","\n","    return df\n","\n","# Read the data\n","df = pd.read_csv(\"/content/drive/MyDrive/NLP Techniques/Car_Details.csv\")\n","\n","# Apply label encoding\n","df = label_encode(df, \"owner\")\n","\n","# Display the resulting DataFrame\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EtfqMGkTrqg1","executionInfo":{"status":"ok","timestamp":1744036737537,"user_tz":240,"elapsed":69,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"53773aa2-e42b-4b9e-fb62-1474ba4e9c36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                       name  year  selling_price  km_driven    fuel  \\\n","0             Maruti 800 AC  2007          60000      70000  Petrol   \n","1  Maruti Wagon R LXI Minor  2007         135000      50000  Petrol   \n","2      Hyundai Verna 1.6 SX  2012         600000     100000  Diesel   \n","3    Datsun RediGO T Option  2017         250000      46000  Petrol   \n","4     Honda Amaze VX i-DTEC  2014         450000     141000  Diesel   \n","\n","  seller_type transmission  owner  \n","0  Individual       Manual      0  \n","1  Individual       Manual      0  \n","2  Individual       Manual      0  \n","3  Individual       Manual      0  \n","4  Individual       Manual      2  \n"]}]},{"cell_type":"markdown","source":["## Pros and Cons of Label Encoding\n","\n","### Pros of Label Encoding\n","\n","- **Simplicity**: Label encoding is straightforward to implement and understand. It involves assigning a unique integer to each category, which is computationally efficient.\n","\n","- **Reduced Dimensionality**: Unlike one-hot encoding, label encoding does not increase the dimensionality of the dataset. This can be beneficial when dealing with high-cardinality categorical variables.\n","\n","- **Memory Efficient**: Label encoding is more memory-efficient compared to one-hot encoding, especially for categorical variables with many unique values.\n","\n","- **Compatibility with Tree-Based Algorithms**: Label encoding works well with tree-based algorithms (like decision trees and random forests) that can handle categorical data natively without assuming any ordinal relationship.\n","\n","### Cons of Label Encoding\n","\n","- **Ordinal Relationships**: Label encoding imposes an ordinal relationship between categories, which may not exist. This can mislead algorithms that assume a natural ordering (like linear regression or k-nearest neighbors).\n","\n","- **Algorithm Sensitivity**: Some algorithms are sensitive to the arbitrary ordering imposed by label encoding, which can lead to poor model performance if the algorithm assumes a linear relationship between the encoded values.\n","\n","- **Interpretability**: The resulting model can be harder to interpret, as the integer values do not have any inherent meaning related to the original categories.\n","\n","- **Not Suitable for All Algorithms**: Label encoding is not suitable for algorithms that assume linearity or require numerical input with meaningful distances between values.\n","\n","### When to Use Label Encoding\n","\n","- Use label encoding when the categorical variable has an inherent order or when using tree-based algorithms that are not affected by the ordinal relationships.\n","- It is particularly useful for reducing dimensionality and memory usage when dealing with high-cardinality categorical variables.\n"],"metadata":{"id":"7ZMqyfe01CiO"}},{"cell_type":"code","source":[],"metadata":{"id":"XuSW1q0XsLDq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bag of Words"],"metadata":{"id":"kkPOa6tE1Llo"}},{"cell_type":"markdown","source":["### Bag of Words --> Using Sklearn"],"metadata":{"id":"uJKzJ8Y114Zc"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Sample corpus\n","corpus = [\n","    'The cat sat on the mat.',\n","    'The dog sat on the log.',\n","    'Dogs and cats are great pets.'\n","]\n","\n","# Create the CountVectorizer object\n","vectorizer = CountVectorizer()\n","\n","# Fit and transform the corpus\n","X = vectorizer.fit_transform(corpus)\n","\n","# Get the feature names (vocabulary)\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Print the feature names and the transformed vectors\n","print(\"Feature Names:\", feature_names)\n","print(\"Bag of Words Representation:\\n\", X.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJUpvX5B1RW2","executionInfo":{"status":"ok","timestamp":1744039314508,"user_tz":240,"elapsed":90,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"2a261243-ba14-4e01-9195-40d79bfb9ac6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature Names: ['and' 'are' 'cat' 'cats' 'dog' 'dogs' 'great' 'log' 'mat' 'on' 'pets'\n"," 'sat' 'the']\n","Bag of Words Representation:\n"," [[0 0 1 0 0 0 0 0 1 1 0 1 2]\n"," [0 0 0 0 1 0 0 1 0 1 0 1 2]\n"," [1 1 0 1 0 1 1 0 0 0 1 0 0]]\n"]}]},{"cell_type":"markdown","source":["### Bag of Words --> From Scratch"],"metadata":{"id":"L5A-c1333LhZ"}},{"cell_type":"code","source":["import numpy as np\n","from collections import Counter\n","import re\n","\n","def tokenize(text):\n","    # Simple tokenizer using regex to find words\n","    return re.findall(r'\\b\\w+\\b', text.lower())\n","\n","def build_vocabulary(corpus):\n","    # Build a set of unique words from the corpus\n","    vocabulary = set()\n","    for document in corpus:\n","        vocabulary.update(tokenize(document))\n","    return sorted(vocabulary)\n","\n","def encode_documents(corpus, vocabulary):\n","    # Encode each document as a vector of word counts\n","    word_index = {word: index for index, word in enumerate(vocabulary)}\n","    features = []\n","    for document in corpus:\n","        word_counts = Counter(tokenize(document))\n","        feature_vector = np.zeros(len(vocabulary))\n","        for word, count in word_counts.items():\n","            if word in word_index:\n","                feature_vector[word_index[word]] = count\n","        features.append(feature_vector)\n","    return np.array(features)\n","\n","# Sample corpus\n","corpus = [\n","    'The cat sat on the mat.',\n","    'The dog sat on the log.',\n","    'Dogs and cats are great pets.'\n","]\n","\n","# Build the vocabulary\n","vocabulary = build_vocabulary(corpus)\n","\n","# Encode the documents\n","features = encode_documents(corpus, vocabulary)\n","\n","# Print the vocabulary and feature vectors\n","print(\"Vocabulary:\", vocabulary)\n","print(\"Bag of Words Representation:\\n\", features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f74kV5Z32ANk","executionInfo":{"status":"ok","timestamp":1744039688387,"user_tz":240,"elapsed":24,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"938dd4ce-6750-401e-d547-d3ad66c21e79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: ['and', 'are', 'cat', 'cats', 'dog', 'dogs', 'great', 'log', 'mat', 'on', 'pets', 'sat', 'the']\n","Bag of Words Representation:\n"," [[0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 2.]\n"," [0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 2.]\n"," [1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["## Pros and Cons of Bag of Words\n","\n","### Pros\n","\n","- **Simplicity**: The BoW model is straightforward to implement and understand. It converts text into numerical features that can be easily used with various machine learning algorithms.\n","\n","- **Efficiency**: It is computationally efficient, especially for large datasets, as it involves simple counting operations.\n","\n","- **Scalability**: The model can scale well with large corpora and is suitable for tasks like document classification and information retrieval.\n","\n","- **Compatibility**: It works well with traditional machine learning algorithms that require fixed-length feature vectors, such as logistic regression, SVMs, and naive Bayes classifiers.\n","\n","- **Baseline Performance**: Despite its simplicity, BoW often provides a strong baseline performance for many text classification tasks.\n","\n","### Cons\n","\n","- **Loss of Context**: The BoW model ignores the order of words, which means it loses contextual information. This can be a significant limitation for tasks where word order is important, such as sentiment analysis or language translation.\n","\n","- **Sparse Representation**: The resulting feature vectors are often sparse, especially with large vocabularies. This can lead to inefficiencies in storage and computation.\n","\n","- **Ignoring Semantics**: It treats all words independently and does not capture semantic relationships between words. For example, \"king\" and \"queen\" are treated as completely different features despite their semantic similarity.\n","\n","- **High Dimensionality**: The dimensionality of the feature space can be very high, equal to the size of the vocabulary. This can lead to the \"curse of dimensionality\" and overfitting, especially with small datasets.\n","\n","- **No Handling of Synonyms and Polysemy**: The model does not handle synonyms (different words with similar meanings) or polysemy (words with multiple meanings) effectively.\n","\n","- **Limited to Known Words**: It can only represent words that are present in the training corpus. Out-of-vocabulary words are not handled well.\n","\n","### Conclusion\n","\n","- The Bag of Words model is a useful starting point for many NLP tasks due to its simplicity and efficiency.\n","- These sentences will be considered similar and will be placed close to each other:\n","\n","      A. This is a very good movie\n","\n","      B. This is not a very good movie\n","- However, its limitations in capturing context and semantics often necessitate the use of more advanced techniques, such as word embeddings (e.g., Word2Vec, GloVe) or transformer-based models (e.g., BERT), for more complex and nuanced language understanding tasks.\n"],"metadata":{"id":"JPd-ToCm34jS"}},{"cell_type":"markdown","source":["# N-grams"],"metadata":{"id":"I_4tD62h5QJG"}},{"cell_type":"markdown","source":["## Implementation using Sklearn"],"metadata":{"id":"2dkLo1Hq5Th6"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","# Sample text data\n","corpus = [\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"Never jump over the lazy dog quickly.\"\n","]\n","\n","# CountVectorizer with n-grams\n","vectorizer = CountVectorizer(ngram_range=(1, 2))  # Unigrams and bigrams\n","X = vectorizer.fit_transform(corpus)\n","\n","# Print the feature names (n-grams)\n","print(\"Feature names (n-grams):\", vectorizer.get_feature_names_out())\n","\n","# Print the vectorized output\n","print(\"Vectorized output:\\n\", X.toarray())\n","\n","# Alternatively, use TF-IDF Vectorizer\n","tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n","X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n","\n","# Print the TF-IDF vectorized output\n","print(\"TF-IDF Vectorized output:\\n\", X_tfidf.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d23qU7CQ3bf6","executionInfo":{"status":"ok","timestamp":1744040225061,"user_tz":240,"elapsed":27,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"f550ff82-61b0-4c2b-9902-467c770afc3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature names (n-grams): ['brown' 'brown fox' 'dog' 'dog quickly' 'fox' 'fox jumps' 'jump'\n"," 'jump over' 'jumps' 'jumps over' 'lazy' 'lazy dog' 'never' 'never jump'\n"," 'over' 'over the' 'quick' 'quick brown' 'quickly' 'the' 'the lazy'\n"," 'the quick']\n","Vectorized output:\n"," [[1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 2 1 1]\n"," [0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0]]\n","TF-IDF Vectorized output:\n"," [[0.26666724 0.26666724 0.18973594 0.         0.26666724 0.26666724\n","  0.         0.         0.26666724 0.26666724 0.18973594 0.18973594\n","  0.         0.         0.18973594 0.18973594 0.26666724 0.26666724\n","  0.         0.37947187 0.18973594 0.26666724]\n"," [0.         0.         0.23031454 0.32369906 0.         0.\n","  0.32369906 0.32369906 0.         0.         0.23031454 0.23031454\n","  0.32369906 0.32369906 0.23031454 0.23031454 0.         0.\n","  0.32369906 0.23031454 0.23031454 0.        ]]\n"]}]},{"cell_type":"markdown","source":["## N-grams --> From Scratch"],"metadata":{"id":"e_nMteQM5_1d"}},{"cell_type":"code","source":["import re\n","from collections import Counter\n","\n","def extract_ngrams(text, n, mode='word', remove_punctuation=True):\n","    \"\"\"\n","    Extracts n-grams from a given text.\n","\n","    Parameters:\n","    - text: str : The input text.\n","    - n: int : The number of items in each n-gram.\n","    - mode: str : 'word' for word n-grams, 'char' for character n-grams.\n","    - remove_punctuation: bool : Whether to remove punctuation from the text.\n","\n","    Returns:\n","    - ngrams: list of tuples : The extracted n-grams.\n","    - ngram_freq: Counter : The frequency count of each n-gram.\n","    \"\"\"\n","\n","    if remove_punctuation:\n","        # Remove punctuation using regex\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","\n","    if mode == 'word':\n","        # Tokenize the text into words\n","        tokens = text.split()\n","    elif mode == 'char':\n","        # Tokenize the text into characters\n","        tokens = list(text)\n","    else:\n","        raise ValueError(\"Mode should be 'word' or 'char'.\")\n","\n","    # Generate n-grams\n","    ngrams = [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]\n","\n","    # Count the frequency of each n-gram\n","    ngram_freq = Counter(ngrams)\n","\n","    return ngrams, ngram_freq\n","\n","# Example usage\n","text = \"The quick brown fox jumps over the lazy dog. The dog is very lazy.\"\n","n = 2  # Bigrams\n","mode = 'word'  # Word n-grams\n","ngrams, ngram_freq = extract_ngrams(text, n, mode, remove_punctuation=True)\n","\n","print(\"Extracted n-grams:\", ngrams)\n","print(\"N-gram frequencies:\", dict(ngram_freq))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PFeLDLdr5ehD","executionInfo":{"status":"ok","timestamp":1744040387160,"user_tz":240,"elapsed":59,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"9a089572-728a-45ae-aec2-13f050f8849b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracted n-grams: [('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog'), ('dog', 'The'), ('The', 'dog'), ('dog', 'is'), ('is', 'very'), ('very', 'lazy')]\n","N-gram frequencies: {('The', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', 'jumps'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', 'The'): 1, ('The', 'dog'): 1, ('dog', 'is'): 1, ('is', 'very'): 1, ('very', 'lazy'): 1}\n"]}]},{"cell_type":"markdown","source":["## Pros and Cons of N-grams\n","\n","### Pros of Using N-grams for Word Embeddings\n","\n","- **Context Capture:**\n","\n","N-grams capture local word order and context, which can be beneficial for tasks that require understanding of word sequences, such as language modeling and machine translation.\n","\n","- **Simplicity:**\n","\n","N-grams are straightforward to implement and understand. They don't require complex neural network architectures or extensive computational resources.\n","\n","- **Flexibility:**\n","\n","You can easily adjust the value of \\( n \\) to capture different levels of context. For example, unigrams capture single words, bigrams capture pairs of words, and so on.\n","\n","### Cons of Using N-grams for Word Embeddings\n","\n","- **Dimensionality:**\n","\n","The dimensionality of the feature space increases exponentially with \\( n \\), leading to high-dimensional and sparse vectors, which can be computationally expensive and challenging to manage.\n","\n","- **Data Sparsity:**\n","\n","As \\( n \\) increases, the number of possible n-grams grows rapidly, leading to data sparsity issues. This can be problematic for tasks with limited training data.\n","\n","- **Lack of Semantic Meaning:**\n","\n","N-grams do not capture semantic relationships between words. For example, \"king\" and \"queen\" might have similar meanings but different n-gram representations.\n","\n","- **Fixed Context Window:**\n","\n","N-grams have a fixed context window size, which may not be sufficient to capture long-range dependencies in text. This can be a limitation for tasks that require understanding of broader context.\n","\n","- **Memory and Storage:**\n","\n","Storing and processing high-dimensional n-gram vectors can be memory-intensive, especially for large text corpora.\n","\n","- **Out-of-Vocabulary Words:**\n","\n","N-grams can struggle with out-of-vocabulary words, as they rely on exact matches of word sequences. This can be mitigated by using character n-grams, but it's still a challenge.\n","\n","### Conclusion\n","\n","N-grams offer a simple and effective way to capture local context in text data, but they come with limitations in terms of dimensionality, data sparsity, and semantic understanding.\n"],"metadata":{"id":"zUPPvRB27F_F"}},{"cell_type":"markdown","source":["# TF-IDF"],"metadata":{"id":"nVz8x2ON8ZRS"}},{"cell_type":"markdown","source":["## Term Frequency-Inverse Document Frequency (TF-IDF)\n","\n","Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It is often used in information retrieval and text mining as a weighting factor in word embeddings.\n","\n","### TF-IDF Components\n","\n","#### Term Frequency (TF)\n","Measures how frequently a term occurs in a document. It is calculated as:\n","\n","$$\n","\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n","$$\n","\n","\n","\n","\n","#### Inverse Document Frequency (IDF)\n","Measures how important a term is across the entire corpus. It is calculated as:\n","\n","$$\n","\\text{IDF}(t, D) = \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t} \\right)\n","$$\n","\n","\n","#### TF-IDF\n","Combines TF and IDF to give a score that highlights the importance of a term in a document relative to the corpus. It is calculated as:\n","\n","$$\n","\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n","$$\n","\n"],"metadata":{"id":"gRIEt0HN8bWV"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Sample corpus\n","documents = [\n","    \"The cat sat on the mat.\",\n","    \"The dog sat on the log.\",\n","    \"Dogs and cats are common pets.\",\n","    \"Cats and dogs are good companions.\"\n","]\n","\n","# Create the TF-IDF Vectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","# Fit and transform the documents\n","tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n","\n","# Get the feature names (words)\n","feature_names = tfidf_vectorizer.get_feature_names_out()\n","\n","# Convert the TF-IDF matrix to a dense format for better readability\n","tfidf_dense = tfidf_matrix.todense()\n","\n","# Output the TF-IDF matrix and feature names\n","print(tfidf_dense)\n","print(feature_names)"],"metadata":{"id":"J4-DAobo6GGN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744066508970,"user_tz":240,"elapsed":3047,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"503da3bf-5c7e-43fe-d59c-b96f0fef6828"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.         0.         0.41777218 0.         0.         0.\n","  0.         0.         0.         0.         0.41777218 0.32937638\n","  0.         0.32937638 0.65875277]\n"," [0.         0.         0.         0.         0.         0.\n","  0.41777218 0.         0.         0.41777218 0.         0.32937638\n","  0.         0.32937638 0.65875277]\n"," [0.37222485 0.37222485 0.         0.37222485 0.47212003 0.\n","  0.         0.37222485 0.         0.         0.         0.\n","  0.47212003 0.         0.        ]\n"," [0.37222485 0.37222485 0.         0.37222485 0.         0.47212003\n","  0.         0.37222485 0.47212003 0.         0.         0.\n","  0.         0.         0.        ]]\n","['and' 'are' 'cat' 'cats' 'common' 'companions' 'dog' 'dogs' 'good' 'log'\n"," 'mat' 'on' 'pets' 'sat' 'the']\n"]}]},{"cell_type":"markdown","source":["## TF-IDF --> From Scratch"],"metadata":{"id":"4Y8Xu_h5eKX7"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from collections import Counter\n","from math import log\n","\n","# Sample corpus\n","documents = [\n","    \"The cat sat on the mat.\",\n","    \"The dog sat on the log.\",\n","    \"Dogs and cats are common pets.\",\n","    \"Cats and dogs are good companions.\"\n","]\n","\n","# Step 1: Calculate Term Frequency (TF)\n","def compute_tf(documents):\n","    tf_matrix = []\n","    for doc in documents:\n","        # Count the frequency of each word in the document\n","        word_count = Counter(doc.split())\n","        total_words = len(doc.split())\n","        # Compute TF for each word\n","        tf_scores = {word: count / total_words for word, count in word_count.items()}\n","        tf_matrix.append(tf_scores)\n","    return tf_matrix\n","\n","# Step 2: Calculate Inverse Document Frequency (IDF)\n","def compute_idf(documents):\n","    idf_scores = {}\n","    total_docs = len(documents)\n","    # Count the number of documents that contain each word\n","    for doc in documents:\n","        words = set(doc.split())\n","        for word in words:\n","            idf_scores[word] = idf_scores.get(word, 0) + 1\n","    # Compute IDF for each word\n","    idf_scores = {word: log(total_docs / count) for word, count in idf_scores.items()}\n","    return idf_scores\n","\n","# Step 3: Compute TF-IDF\n","def compute_tfidf(tf_matrix, idf_scores):\n","    tfidf_matrix = []\n","    for tf_scores in tf_matrix:\n","        tfidf_scores = {word: tf * idf_scores.get(word, 0) for word, tf in tf_scores.items()}\n","        tfidf_matrix.append(tfidf_scores)\n","    return tfidf_matrix\n","\n","# Compute TF, IDF, and TF-IDF\n","tf_matrix = compute_tf(documents)\n","idf_scores = compute_idf(documents)\n","tfidf_matrix = compute_tfidf(tf_matrix, idf_scores)\n","\n","# Convert the TF-IDF matrix to a DataFrame for better readability\n","tfidf_df = pd.DataFrame(tfidf_matrix).fillna(0)\n","tfidf_df.columns = sorted(idf_scores.keys())\n","tfidf_df = tfidf_df.T\n","\n","tfidf_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":613},"id":"hEWbXB93drFM","executionInfo":{"status":"ok","timestamp":1744066646333,"user_tz":240,"elapsed":83,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"7b614b34-5ea9-479b-c651-d78523d1823f"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                    0         1         2         3\n","Cats         0.115525  0.115525  0.000000  0.000000\n","Dogs         0.231049  0.000000  0.000000  0.000000\n","The          0.115525  0.115525  0.000000  0.000000\n","and          0.115525  0.115525  0.000000  0.000000\n","are          0.115525  0.115525  0.000000  0.000000\n","cat          0.231049  0.000000  0.000000  0.000000\n","cats         0.000000  0.231049  0.000000  0.000000\n","common       0.000000  0.231049  0.000000  0.000000\n","companions.  0.000000  0.000000  0.231049  0.000000\n","dog          0.000000  0.000000  0.115525  0.115525\n","dogs         0.000000  0.000000  0.231049  0.000000\n","good         0.000000  0.000000  0.115525  0.115525\n","log.         0.000000  0.000000  0.231049  0.000000\n","mat.         0.000000  0.000000  0.231049  0.000000\n","on           0.000000  0.000000  0.000000  0.231049\n","pets.        0.000000  0.000000  0.000000  0.231049\n","sat          0.000000  0.000000  0.000000  0.231049\n","the          0.000000  0.000000  0.000000  0.231049"],"text/html":["\n","  <div id=\"df-f7950fe9-a99b-452b-be8a-c658e970b709\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Cats</th>\n","      <td>0.115525</td>\n","      <td>0.115525</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Dogs</th>\n","      <td>0.231049</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>The</th>\n","      <td>0.115525</td>\n","      <td>0.115525</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>and</th>\n","      <td>0.115525</td>\n","      <td>0.115525</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>are</th>\n","      <td>0.115525</td>\n","      <td>0.115525</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>cat</th>\n","      <td>0.231049</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>cats</th>\n","      <td>0.000000</td>\n","      <td>0.231049</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>common</th>\n","      <td>0.000000</td>\n","      <td>0.231049</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>companions.</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.231049</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>dog</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.115525</td>\n","      <td>0.115525</td>\n","    </tr>\n","    <tr>\n","      <th>dogs</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.231049</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>good</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.115525</td>\n","      <td>0.115525</td>\n","    </tr>\n","    <tr>\n","      <th>log.</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.231049</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mat.</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.231049</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>on</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.231049</td>\n","    </tr>\n","    <tr>\n","      <th>pets.</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.231049</td>\n","    </tr>\n","    <tr>\n","      <th>sat</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.231049</td>\n","    </tr>\n","    <tr>\n","      <th>the</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.231049</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7950fe9-a99b-452b-be8a-c658e970b709')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f7950fe9-a99b-452b-be8a-c658e970b709 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f7950fe9-a99b-452b-be8a-c658e970b709');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-796a20de-efef-4d24-814d-7bc112dc5c51\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-796a20de-efef-4d24-814d-7bc112dc5c51')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-796a20de-efef-4d24-814d-7bc112dc5c51 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_b19aa4f0-3d92-450d-8564-e81731e046e1\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('tfidf_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_b19aa4f0-3d92-450d-8564-e81731e046e1 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('tfidf_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"tfidf_df","summary":"{\n  \"name\": \"tfidf_df\",\n  \"rows\": 18,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08142078615823126,\n        \"min\": 0.0,\n        \"max\": 0.23104906018664842,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.11552453009332421,\n          0.23104906018664842,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08142078615823126,\n        \"min\": 0.0,\n        \"max\": 0.23104906018664842,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.11552453009332421,\n          0.0,\n          0.23104906018664842\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09884108556601129,\n        \"min\": 0.0,\n        \"max\": 0.23104906018664842,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          0.23104906018664842,\n          0.11552453009332421\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0988410855660113,\n        \"min\": 0.0,\n        \"max\": 0.23104906018664842,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          0.11552453009332421,\n          0.23104906018664842\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["## Pros and Cons of TF-IDF\n","\n","### Pros of TF-IDF\n","\n","- **Simplicity**: TF-IDF is straightforward to implement and understand. It's based on simple mathematical concepts and doesn't require complex computations.\n","\n","- **Effectiveness**: It's effective for many text mining and information retrieval tasks, such as keyword extraction, document similarity, and feature extraction for machine learning models.\n","\n","- **Scalability**: TF-IDF can handle large datasets efficiently. It scales well with the size of the corpus and the number of documents.\n","\n","- **Reduces Dimensionality**: By focusing on important words, TF-IDF helps reduce the dimensionality of the feature space, which is beneficial for machine learning algorithms.\n","\n","- **Interpretability**: The resulting scores are interpretable, making it easy to understand the importance of words in a document relative to the corpus.\n","\n","### Cons of TF-IDF\n","\n","- **Sparse Representation**: TF-IDF matrices are often sparse, especially with large vocabularies. This can lead to inefficiencies in storage and computation.\n","\n","- **Ignoring Semantics**: TF-IDF does not capture the semantic meaning of words. It treats words independently and does not consider the context or relationships between words.\n","\n","- **Sensitivity to Corpus Size**: The IDF component is sensitive to the size and nature of the corpus. Adding or removing documents can change the IDF values significantly.\n","\n","- **Assumes Word Independence**: TF-IDF assumes that words are independent, which is not always true in natural language. It doesn't account for phrases or multi-word expressions.\n","\n","- **Ignores Word Order**: TF-IDF does not consider the order of words in a document, which can be crucial for understanding the meaning of text.\n","\n","- **Not Suitable for Short Texts**: TF-IDF may not perform well with very short texts, as there is not enough context to determine the importance of words accurately.\n"],"metadata":{"id":"d-kQ1r9Fe5bj"}},{"cell_type":"markdown","source":["# **Word2Vec**"],"metadata":{"id":"qZH3ODB3ik85"}},{"cell_type":"markdown","source":["Word2Vec is a popular technique used in natural language processing (NLP) to create word embeddings, which are dense vector representations of words. These embeddings capture semantic similarity between words, meaning that words with similar meanings will have similar vector representations. Word2Vec was developed by Tomas Mikolov and his team at Google in 2013.\n","\n","## Key Concepts of Word2Vec\n","\n","### Word Embeddings\n","\n","- Word embeddings are numerical representations of words in a continuous vector space.\n","- They capture semantic and syntactic similarity, meaning that words with similar meanings or grammatical roles will have similar vector representations.\n","\n","### Architecture\n","\n","Word2Vec uses a neural network model to learn word associations from a large corpus of text. There are two main architectures: Continuous Bag of Words (CBOW) and Skip-gram.\n","\n","### Training Process\n","\n","- The model is trained using a large corpus of text.\n","- During training, the model adjusts the weights (embeddings) to maximize the probability of predicting the correct words.\n","- The training objective is to maximize the likelihood of the context words given the target word (Skip-gram) or vice versa (CBOW).\n","\n","### Vector Space\n","\n","- The resulting word vectors are positioned in a high-dimensional space (typically 100-300 dimensions).\n","- Words with similar meanings are closer to each other in this vector space.\n","- Mathematical operations can be performed on these vectors to capture semantic relationships (e.g., \"King\" - \"Man\" + \"Woman\" ≈ \"Queen\").\n","\n","### Applications\n","\n","- Word2Vec embeddings are used in various NLP tasks such as sentiment analysis, machine translation, and text classification.\n","- They provide a way to represent words in a format that can be used as input for machine learning models.\n","\n","### Limitations\n","\n","- Word2Vec does not capture polysemy (multiple meanings of a word) well, as it generates a single vector for each word.\n","- It does not consider the context beyond the window size.\n","- It does not handle out-of-vocabulary words (words not seen during training).\n","\n","## Example\n","\n","Consider the sentence: \"The quick brown fox jumps over the lazy dog.\"\n","\n","- In CBOW, given the context words [\"The\", \"brown\", \"jumps\"], the model predicts the target word \"fox\".\n","- In Skip-gram, given the target word \"fox\", the model predicts the context words [\"The\", \"brown\", \"jumps\"].\n","\n","Word2Vec has been instrumental in advancing the field of NLP by providing a way to represent words in a continuous vector space that captures semantic meaning.\n"],"metadata":{"id":"GnFQXJBHipeH"}},{"cell_type":"markdown","source":["## **Continuous Bag of Words (CBOW)**"],"metadata":{"id":"CjTH2jnLjS3W"}},{"cell_type":"markdown","source":["### Objective\n","\n","- The CBOW model aims to predict a target word from a window of surrounding context words.\n","- It maximizes the probability of the target word given the context words.\n","\n","### Architecture\n","\n","- The input to the model is a set of context words, and the output is the target word.\n","- The context words are represented as a \"bag of words,\" meaning the order of words does not matter.\n","- The model uses a neural network with a single hidden layer to predict the target word.\n","\n","### Training\n","\n","- During training, the model adjusts the weights (word vectors) to maximize the likelihood of predicting the correct target word.\n","- The context words are fed into the network, and the output is compared to the actual target word.\n","- The error is backpropagated to update the word vectors.\n","\n","### Advantages\n","\n","- CBOW is faster to train compared to Skip-gram because it predicts only one word at a time.\n","- It works well with smaller datasets.\n","\n","### Disadvantages\n","\n","- CBOW may not capture rare words as effectively as Skip-gram because it relies on the context words to predict the target word.\n"],"metadata":{"id":"f4SrHdymjkOM"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Sample data\n","sentences = [\n","    ['the', 'quick', 'brown', 'fox'],\n","    ['jumps', 'over', 'the', 'lazy', 'dog']\n","]\n","\n","# Hyperparameters\n","window_size = 2\n","embedding_dim = 5\n","epochs = 1000\n","learning_rate = 0.05\n","\n","# Preprocess data\n","label_encoder = LabelEncoder()\n","all_words = [word for sentence in sentences for word in sentence]\n","label_encoder.fit(all_words)\n","vocab_size = len(label_encoder.classes_)\n","\n","# Initialize weights\n","W1 = np.random.rand(vocab_size, embedding_dim)\n","W2 = np.random.rand(embedding_dim, vocab_size)\n","\n","# Training CBOW\n","for epoch in range(epochs):\n","    for sentence in sentences:\n","        for target_idx in range(len(sentence)):\n","            context_words = []\n","            for j in range(max(0, target_idx - window_size), min(len(sentence), target_idx + window_size + 1)):\n","                if j != target_idx:\n","                    context_words.append(sentence[j])\n","\n","            if not context_words:\n","                continue\n","\n","            context_indices = label_encoder.transform(context_words)\n","            target_index = label_encoder.transform([sentence[target_idx]])[0]\n","\n","            # Forward pass\n","            context_vectors = W1[context_indices]\n","            hidden_layer = np.mean(context_vectors, axis=0)\n","            output_layer = np.dot(W2.T, hidden_layer)\n","\n","            # Softmax to get probabilities\n","            exp_output_layer = np.exp(output_layer - np.max(output_layer))\n","            probs = exp_output_layer / np.sum(exp_output_layer)\n","\n","            # Backpropagation\n","            error = probs\n","            error[target_index] -= 1\n","            W2[:, context_indices] -= learning_rate * np.outer(hidden_layer, error[context_indices])\n","            W1[context_indices, :] -= learning_rate * np.outer(error[context_indices], hidden_layer)\n","\n","# Output the word embeddings\n","word_embeddings = {word: W1[idx] for word, idx in zip(label_encoder.classes_, range(vocab_size))}\n","print(\"CBOW Word Embeddings:\", word_embeddings)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ga9758rweQ__","executionInfo":{"status":"ok","timestamp":1744068167964,"user_tz":240,"elapsed":11066,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"467671e8-7344-4894-886c-8b79db7a12e6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["CBOW Word Embeddings: {np.str_('brown'): array([-0.00904331, -0.05148498, -0.05104738, -0.0195077 , -0.01440888]), np.str_('dog'): array([ 0.11481653, -0.05281729,  0.04417411, -0.18932073,  0.02775146]), np.str_('fox'): array([-0.07818388,  0.09821605, -0.04733932,  0.16028635,  0.05183857]), np.str_('jumps'): array([ 0.12595104,  0.03393909,  0.06901787,  0.05164945, -0.06299382]), np.str_('lazy'): array([-0.14233399,  0.01974778, -0.07652036,  0.05914171,  0.05738064]), np.str_('over'): array([-0.13777552,  0.06394807, -0.06190092,  0.17926208,  0.01165647]), np.str_('quick'): array([0.00428204, 0.03786512, 0.05040596, 0.00489799, 0.00931386]), np.str_('the'): array([ 0.09059586, -0.0571445 ,  0.04824039, -0.1150323 , -0.03578787])}\n"]}]},{"cell_type":"markdown","source":["## **Skip-gram**"],"metadata":{"id":"fJb-DEbhkKCp"}},{"cell_type":"markdown","source":["### Objective\n","\n","- The Skip-gram model aims to predict the context words given a target word.\n","- It maximizes the probability of the context words given the target word.\n","\n","### Architecture\n","\n","- The input to the model is the target word, and the output is the context words within a certain window size.\n","- The model uses a neural network with a single hidden layer to predict multiple context words.\n","\n","### Training\n","\n","- During training, the model adjusts the weights (word vectors) to maximize the likelihood of predicting the correct context words.\n","- The target word is fed into the network, and the output is compared to the actual context words.\n","- The error is backpropagated to update the word vectors.\n","\n","### Advantages\n","\n","- Skip-gram works better with large datasets and captures rare words more effectively.\n","- It can handle a larger vocabulary size compared to CBOW.\n","\n","### Disadvantages\n","\n","- Skip-gram is slower to train compared to CBOW because it predicts multiple words at a time.\n"],"metadata":{"id":"HXTv73NokRI8"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Sample data\n","sentences = [\n","    ['the', 'quick', 'brown', 'fox'],\n","    ['jumps', 'over', 'the', 'lazy', 'dog']\n","]\n","\n","# Hyperparameters\n","window_size = 2\n","embedding_dim = 5\n","epochs = 1000\n","learning_rate = 0.05\n","\n","# Preprocess data\n","label_encoder = LabelEncoder()\n","all_words = [word for sentence in sentences for word in sentence]\n","label_encoder.fit(all_words)\n","vocab_size = len(label_encoder.classes_)\n","\n","# Initialize weights\n","W1 = np.random.rand(vocab_size, embedding_dim)\n","W2 = np.random.rand(embedding_dim, vocab_size)\n","\n","# Training Skip-gram\n","for epoch in range(epochs):\n","    for sentence in sentences:\n","        for target_idx in range(len(sentence)):\n","            target_word = sentence[target_idx]\n","            target_index = label_encoder.transform([target_word])[0]\n","\n","            context_words = []\n","            for j in range(max(0, target_idx - window_size), min(len(sentence), target_idx + window_size + 1)):\n","                if j != target_idx:\n","                    context_words.append(sentence[j])\n","\n","            if not context_words:\n","                continue\n","\n","            context_indices = label_encoder.transform(context_words)\n","\n","            # Forward pass\n","            target_vector = W1[target_index]\n","            output_layer = np.dot(W2.T, target_vector)\n","\n","            # Softmax to get probabilities\n","            exp_output_layer = np.exp(output_layer - np.max(output_layer))\n","            probs = exp_output_layer / np.sum(exp_output_layer)\n","\n","            # Backpropagation\n","            error = probs.copy()\n","            for context_index in context_indices:\n","                error[context_index] -= 1\n","                # Update weights\n","                W2[:, context_index] -= learning_rate * target_vector\n","                W1[target_index, :] -= learning_rate * error[context_index] * W2[:, context_index]\n","\n","# Output the word embeddings\n","word_embeddings = {word: W1[idx] for word, idx in zip(label_encoder.classes_, range(vocab_size))}\n","print(\"Skip-gram Word Embeddings:\", word_embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kGSRHHPcjyTy","executionInfo":{"status":"ok","timestamp":1744068548607,"user_tz":240,"elapsed":3765,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"e9ced839-ed49-4a58-902f-f4e9458d8baf"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Skip-gram Word Embeddings: {np.str_('brown'): array([ 0.09126157, -0.0405691 ,  0.3118307 ,  0.09852563, -0.00686632]), np.str_('dog'): array([ 0.12044257, -0.0464454 , -0.15187716,  0.08494878,  0.09691155]), np.str_('fox'): array([0.36346851, 0.35255664, 0.14547659, 0.33697078, 0.1187854 ]), np.str_('jumps'): array([ 0.25497921,  0.17467231, -0.25533569, -0.36104801,  0.29881799]), np.str_('lazy'): array([0.34726189, 0.13480416, 0.31155007, 0.0880946 , 0.08570625]), np.str_('over'): array([-0.06939735,  0.03165416, -0.11039496,  0.30406574,  0.46997456]), np.str_('quick'): array([ 0.23636718, -0.2558761 ,  0.10151488,  0.14332214,  0.32082459]), np.str_('the'): array([0.25066364, 0.23073517, 0.50548869, 0.16661775, 0.56623956])}\n"]}]},{"cell_type":"markdown","source":["### Comparison of CBOW and Skip-gram\n","\n","#### Context Window\n","\n","- Both models use a context window to define the range of context words.\n","- The window size can be adjusted to include more or fewer context words.\n","\n","#### Training Time\n","\n","- CBOW is generally faster to train because it predicts only one word at a time.\n","- Skip-gram predicts multiple words, which makes it slower to train.\n","\n","#### Word Frequency\n","\n","- Skip-gram is better at capturing rare words because it uses the target word to predict the context words, which can include rare words.\n","\n","#### Use Cases\n","\n","- CBOW is suitable for smaller datasets and when training speed is a concern.\n","- Skip-gram is better for large datasets and capturing a broader range of word meanings.\n","\n","Both CBOW and Skip-gram are fundamental techniques in Word2Vec for creating word embeddings that capture semantic meaning. The choice between the two depends on the specific requirements and constraints of the task at hand.\n"],"metadata":{"id":"JxnUXefllpLh"}},{"cell_type":"markdown","source":["# GloVe: Global Vectors for Word Representation\n","\n","GloVe, which stands for Global Vectors for Word Representation, is an unsupervised learning algorithm developed by Stanford researchers to generate word embeddings. Word embeddings are vector representations of words that capture semantic similarity, meaning that words with similar meanings will have similar vector representations.\n","\n","## Key Concepts of GloVe\n","\n","- **Global Context**: Unlike some other embedding methods that focus on local context windows, GloVe considers the global context of words in a corpus. It constructs a word-context co-occurrence matrix from the entire text corpus, which captures how frequently words co-occur with each other.\n","\n","- **Co-occurrence Matrix**: GloVe builds a co-occurrence matrix \\( X \\), where each element \\( X_{ij} \\) represents the number of times word \\( j \\) appears in the context of word \\( i \\). The context is typically defined by a window of words around the target word.\n","\n","- **Objective Function**: GloVe aims to minimize the difference between the predicted and actual co-occurrence probabilities. The objective function is designed to capture the ratio of co-occurrence probabilities, which helps in capturing meaningful linear substructures in the word vector space.\n","\n","- **Dimensionality Reduction**: The co-occurrence matrix is factorized to reduce its dimensionality. This process yields two sets of word vectors: one for words as \"center\" words and one for words as \"context\" words. The final word vectors are typically the sum of these two sets.\n","\n","- **Efficiency**: GloVe is computationally efficient and can be trained on large corpora. It leverages statistical information across the entire text corpus, making it effective for capturing global word-word co-occurrence information.\n","\n","## Advantages of GloVe\n","\n","- **Captures Global Statistics**: By considering the entire corpus, GloVe captures global statistical information, which can be beneficial for capturing the meaning of words.\n","- **Efficient Training**: GloVe can be trained relatively quickly on large datasets due to its use of matrix factorization techniques.\n","- **High-Quality Embeddings**: GloVe produces high-quality word embeddings that capture semantic relationships between words.\n","\n","## Applications\n","\n","- **Natural Language Processing (NLP)**: GloVe embeddings are widely used in various NLP tasks such as sentiment analysis, machine translation, and text classification.\n","- **Semantic Similarity**: They can be used to measure the semantic similarity between words, which is useful in information retrieval and recommendation systems.\n","\n","Overall, GloVe is a powerful tool for generating word embeddings that capture the semantic relationships between words, leveraging global statistical information from the text corpus.\n"],"metadata":{"id":"Mcd7mrkRnD79"}},{"cell_type":"code","source":["import numpy as np\n","\n","class GloVe:\n","    def __init__(self, corpus, vector_size=50, window_size=10, learning_rate=0.05, epochs=50):\n","        self.corpus = corpus\n","        self.vector_size = vector_size\n","        self.window_size = window_size\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        self.vocab = self.build_vocab()\n","        self.cooccurrence_matrix = self.build_cooccurrence_matrix()\n","        self.W = np.random.rand(len(self.vocab), vector_size)\n","        self.C = np.random.rand(len(self.vocab), vector_size)\n","        self.bias_w = np.random.rand(len(self.vocab))\n","        self.bias_c = np.random.rand(len(self.vocab))\n","\n","    def build_vocab(self): # This method constructs a vocabulary from the corpus.\n","        vocab = {}\n","        for sentence in self.corpus:\n","            for word in sentence:\n","                if word not in vocab:\n","                    vocab[word] = len(vocab)\n","        return vocab\n","\n","    def build_cooccurrence_matrix(self): # This method constructs a co-occurrence matrix based on the context window size.\n","        cooccurrence_matrix = np.zeros((len(self.vocab), len(self.vocab)))\n","        for sentence in self.corpus:\n","            for i, center_word in enumerate(sentence):\n","                context_words = sentence[max(0, i - self.window_size): i] + sentence[i + 1: min(len(sentence), i + self.window_size + 1)]\n","                center_idx = self.vocab[center_word]\n","                for context_word in context_words:\n","                    context_idx = self.vocab[context_word]\n","                    cooccurrence_matrix[center_idx][context_idx] += 1.0 / len(context_words)\n","        return cooccurrence_matrix\n","\n","    def train(self): # This method updates the word vectors and biases using the GloVe objective function.\n","        for epoch in range(self.epochs):\n","            for i in range(len(self.vocab)):\n","                for j in range(len(self.vocab)):\n","                    if i != j and self.cooccurrence_matrix[i][j] > 0:\n","                        weight = min(1.0, (self.cooccurrence_matrix[i][j] / 100) ** 0.75)\n","                        cost_inner = np.dot(self.W[i], self.C[j]) + self.bias_w[i] + self.bias_c[j] - np.log(self.cooccurrence_matrix[i][j])\n","                        grad_w = weight * cost_inner * self.C[j]\n","                        grad_c = weight * cost_inner * self.W[i]\n","                        self.W[i] -= self.learning_rate * grad_w\n","                        self.C[j] -= self.learning_rate * grad_c\n","                        self.bias_w[i] -= self.learning_rate * weight * cost_inner\n","                        self.bias_c[j] -= self.learning_rate * weight * cost_inner\n","\n","    def get_embeddings(self): # This method returns the final word embeddings by averaging the word vectors and context vectors.\n","        return (self.W + self.C) / 2\n","\n","# Example usage:\n","corpus = [\n","    [\"i\", \"like\", \"machine\", \"learning\"],\n","    [\"i\", \"love\", \"cats\"],\n","    [\"i\", \"enjoy\", \"learning\", \"new\", \"things\"]\n","]\n","\n","glove = GloVe(corpus, vector_size=5, window_size=2, learning_rate=0.05, epochs=100)\n","glove.train()\n","embeddings = glove.get_embeddings()\n","\n","print(\"Word Embeddings:\\n\", embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LInwsCcdk2hz","executionInfo":{"status":"ok","timestamp":1744069148209,"user_tz":240,"elapsed":106,"user":{"displayName":"lalit wale","userId":"09862772144538630739"}},"outputId":"b2a6c9ac-d154-456d-b9e5-6dbd9f2ee0e9"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Embeddings:\n"," [[ 0.17939874  0.41627203  0.01563113  0.23874276  0.00279134]\n"," [ 0.3306892  -0.129326    0.21291018  0.03439553  0.6701372 ]\n"," [ 0.29421412  0.33080237  0.14943978  0.64166909  0.29803278]\n"," [ 0.41879369  0.20771939 -0.1807786  -0.05692185  0.15818512]\n"," [ 0.18601556  0.42714543  0.34674826  0.32768166  0.17758825]\n"," [ 0.02458444  0.28506487  0.4131357   0.57836569  0.18452116]\n"," [ 0.40176779  0.13027695  0.60868793  0.33681591  0.75884909]\n"," [ 0.02119305  0.51771323  0.09235924  0.30762168  0.1090183 ]\n"," [ 0.14177566  0.72977264  0.71460173  0.38812816  0.62755391]]\n"]}]},{"cell_type":"markdown","source":["### Pros and Cons\n","\n","GloVe (Global Vectors for Word Representation) is a popular method for generating word embeddings, and it has its own set of advantages and disadvantages. Here are some of the key pros and cons:\n","\n","#### Pros of GloVe:\n","\n","- **Efficient Training**: GloVe is designed to be computationally efficient. It leverages statistical information from a co-occurrence matrix, which can be precomputed, making the training process faster compared to some other methods like Skip-gram with Negative Sampling (SGNS).\n","\n","- **Global Context**: GloVe considers the global context of words in the entire corpus, not just local context windows. This can lead to more robust embeddings that capture broader semantic relationships.\n","\n","- **Scalability**: GloVe can scale well to large corpora due to its efficient use of matrix factorization techniques.\n","\n","- **Interpretability**: The co-occurrence matrix and the resulting embeddings can be more interpretable, as they directly capture the statistical relationships between words.\n","\n","- **Pre-trained Embeddings**: Pre-trained GloVe embeddings are widely available and have been trained on large corpora, making them easy to use for various NLP tasks.\n","\n","#### Cons of GloVe:\n","\n","- **Memory Usage**: The co-occurrence matrix can be very large, especially for large vocabularies, leading to high memory usage.\n","\n","- **Less Suitable for Rare Words**: GloVe may not perform as well for rare words, as it relies on co-occurrence statistics that may be sparse for infrequent words.\n","\n","- **Static Embeddings**: Like other traditional embedding methods, GloVe produces static embeddings that do not change with context. This can be a limitation compared to contextual embeddings produced by models like BERT.\n","\n","- **Dependence on Corpus**: The quality of GloVe embeddings is highly dependent on the quality and size of the training corpus. Small or poorly curated corpora can lead to less effective embeddings.\n","\n","- **Less Flexible**: GloVe is less flexible compared to neural network-based methods that can be fine-tuned for specific tasks. GloVe embeddings are typically used as-is after training.\n","\n","- **No Subword Information**: GloVe does not inherently capture subword information, which can be a disadvantage for languages with rich morphology or for handling out-of-vocabulary words.\n","\n","In summary, GloVe is a powerful and efficient method for generating word embeddings, particularly suitable for tasks where global context is important. However, it has limitations related to memory usage, handling of rare words, and the static nature of the embeddings.\n"],"metadata":{"id":"W7DuUsj2oumj"}},{"cell_type":"code","source":[],"metadata":{"id":"nqjHUZKvnale"},"execution_count":null,"outputs":[]}]}